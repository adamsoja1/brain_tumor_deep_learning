{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015a3ef8-a3f8-47ee-9ef5-505bd8252967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,Dropout\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec151756-2576-4c01-8392-affb3ee27ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir  = 'data/train'\n",
    "\n",
    "base_dir = 'data/cats_and_dogs'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "train_dir  = os.path.join(base_dir,'train')\n",
    "test_dir  = os.path.join(base_dir,'test')\n",
    "valid_dir  = os.path.join(base_dir,'valid')\n",
    "\n",
    "for directory in (train_dir, valid_dir, test_dir):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "        \n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "\n",
    "valid_cats_dir = os.path.join(valid_dir, 'cats')\n",
    "valid_dogs_dir = os.path.join(valid_dir, 'dogs')\n",
    "\n",
    "\n",
    "dirs = [train_cats_dir,train_dogs_dir, test_cats_dir, test_dogs_dir, valid_cats_dir, valid_dogs_dir ]\n",
    "for directory in dirs:\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9c202f-67ea-4c8b-952b-e8d6c858e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = ['cat.{}.jpg'.format(i) for i in range(9000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(train_cats_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(10000,12000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(valid_cats_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(test_cats_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "#dogs:\n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(9000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(train_dogs_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(10000,12000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(valid_dogs_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir,fname)\n",
    "    dst = os.path.join(test_dogs_dir,fname)\n",
    "    shutil.copyfile(src,dst)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b6eed-0e31-437f-85ae-e28b075104dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62759ebe-bb21-4ccc-b536-ce7ff2cfc830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 147, 147, 32)      1568      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 146, 146, 32)      4128      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 47, 47, 64)        8256      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 44, 44, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 256)       262400    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 256)         1048832   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 756)               774900    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 757       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,166,441\n",
      "Trainable params: 2,166,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(4,4),activation = 'relu',input_shape=(150,150,3)))\n",
    "model.add(Conv2D(filters=32,kernel_size=(2,2),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(filters=64,kernel_size=(2,2),activation = 'relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=(4,4),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(filters=256,kernel_size=(4,4),activation = 'relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=(4,4),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 756,activation = 'relu'))\n",
    "model.add(Dense(units = 1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb4dcde-9713-4137-9a0a-3ef5c41774ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14df0417-aa95-4322-af8b-92bd6d402a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 2 classes.\n",
      "Found 4000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                rotation_range=40,\n",
    "                                width_shift_range=0.3,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip = True)\n",
    "                                   \n",
    "                                  \n",
    "valid_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                                    target_size=(150,150),\n",
    "                                                    batch_size=30,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(directory=valid_dir,\n",
    "                                                    target_size=(150,150),\n",
    "                                                    batch_size = 30,\n",
    "                                                    class_mode ='binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0fe4543-f81c-416f-9458-e4b7e91e63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "600/600 [==============================] - 109s 175ms/step - loss: 0.6720 - accuracy: 0.5821 - val_loss: 0.6929 - val_accuracy: 0.5598\n",
      "Epoch 2/600\n",
      "600/600 [==============================] - 101s 169ms/step - loss: 0.6209 - accuracy: 0.6536 - val_loss: 0.5696 - val_accuracy: 0.7065\n",
      "Epoch 3/600\n",
      "600/600 [==============================] - 101s 168ms/step - loss: 0.5776 - accuracy: 0.6961 - val_loss: 0.5250 - val_accuracy: 0.7412\n",
      "Epoch 4/600\n",
      "600/600 [==============================] - 111s 186ms/step - loss: 0.5479 - accuracy: 0.7210 - val_loss: 0.4847 - val_accuracy: 0.7653\n",
      "Epoch 5/600\n",
      "600/600 [==============================] - 101s 169ms/step - loss: 0.5276 - accuracy: 0.7376 - val_loss: 0.4487 - val_accuracy: 0.7885\n",
      "Epoch 6/600\n",
      "600/600 [==============================] - 100s 167ms/step - loss: 0.5096 - accuracy: 0.7487 - val_loss: 0.4583 - val_accuracy: 0.7878\n",
      "Epoch 7/600\n",
      "600/600 [==============================] - 99s 165ms/step - loss: 0.4820 - accuracy: 0.7671 - val_loss: 0.4378 - val_accuracy: 0.8023\n",
      "Epoch 8/600\n",
      "600/600 [==============================] - 98s 163ms/step - loss: 0.4541 - accuracy: 0.7836 - val_loss: 0.3998 - val_accuracy: 0.8245\n",
      "Epoch 9/600\n",
      "600/600 [==============================] - 97s 162ms/step - loss: 0.4412 - accuracy: 0.7903 - val_loss: 0.4847 - val_accuracy: 0.7840\n",
      "Epoch 10/600\n",
      "600/600 [==============================] - 98s 164ms/step - loss: 0.4221 - accuracy: 0.8057 - val_loss: 0.3417 - val_accuracy: 0.8637\n",
      "Epoch 11/600\n",
      "600/600 [==============================] - 98s 163ms/step - loss: 0.3990 - accuracy: 0.8174 - val_loss: 0.3264 - val_accuracy: 0.8585\n",
      "Epoch 12/600\n",
      "600/600 [==============================] - 98s 163ms/step - loss: 0.3849 - accuracy: 0.8258 - val_loss: 0.3261 - val_accuracy: 0.8660\n",
      "Epoch 13/600\n",
      "600/600 [==============================] - 99s 164ms/step - loss: 0.3664 - accuracy: 0.8314 - val_loss: 0.2914 - val_accuracy: 0.8798\n",
      "Epoch 14/600\n",
      "600/600 [==============================] - 99s 164ms/step - loss: 0.3586 - accuracy: 0.8379 - val_loss: 0.2953 - val_accuracy: 0.8735\n",
      "Epoch 15/600\n",
      "600/600 [==============================] - 96s 161ms/step - loss: 0.3427 - accuracy: 0.8470 - val_loss: 0.2775 - val_accuracy: 0.8848\n",
      "Epoch 16/600\n",
      "600/600 [==============================] - 94s 156ms/step - loss: 0.3396 - accuracy: 0.8526 - val_loss: 0.2708 - val_accuracy: 0.8835\n",
      "Epoch 17/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3292 - accuracy: 0.8523 - val_loss: 0.2721 - val_accuracy: 0.8915\n",
      "Epoch 18/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3167 - accuracy: 0.8603 - val_loss: 0.2717 - val_accuracy: 0.8903\n",
      "Epoch 19/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3153 - accuracy: 0.8601 - val_loss: 0.2527 - val_accuracy: 0.8940\n",
      "Epoch 20/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.3029 - accuracy: 0.8700 - val_loss: 0.2464 - val_accuracy: 0.8960\n",
      "Epoch 21/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3006 - accuracy: 0.8687 - val_loss: 0.2336 - val_accuracy: 0.9038\n",
      "Epoch 22/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2919 - accuracy: 0.8747 - val_loss: 0.2280 - val_accuracy: 0.9080\n",
      "Epoch 23/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2830 - accuracy: 0.8771 - val_loss: 0.2331 - val_accuracy: 0.9115\n",
      "Epoch 24/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2811 - accuracy: 0.8786 - val_loss: 0.2349 - val_accuracy: 0.9087\n",
      "Epoch 25/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2716 - accuracy: 0.8848 - val_loss: 0.2334 - val_accuracy: 0.9065\n",
      "Epoch 26/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2754 - accuracy: 0.8818 - val_loss: 0.2485 - val_accuracy: 0.9003\n",
      "Epoch 27/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2755 - accuracy: 0.8806 - val_loss: 0.2928 - val_accuracy: 0.8723\n",
      "Epoch 28/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2718 - accuracy: 0.8828 - val_loss: 0.2271 - val_accuracy: 0.9190\n",
      "Epoch 29/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2605 - accuracy: 0.8905 - val_loss: 0.3237 - val_accuracy: 0.8930\n",
      "Epoch 30/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2630 - accuracy: 0.8865 - val_loss: 0.2148 - val_accuracy: 0.9225\n",
      "Epoch 31/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2572 - accuracy: 0.8928 - val_loss: 0.2012 - val_accuracy: 0.9230\n",
      "Epoch 32/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2546 - accuracy: 0.8919 - val_loss: 0.2326 - val_accuracy: 0.9095\n",
      "Epoch 33/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2615 - accuracy: 0.8898 - val_loss: 0.2262 - val_accuracy: 0.9160\n",
      "Epoch 34/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2577 - accuracy: 0.8909 - val_loss: 0.2383 - val_accuracy: 0.9135\n",
      "Epoch 35/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2601 - accuracy: 0.8901 - val_loss: 0.2191 - val_accuracy: 0.9097\n",
      "Epoch 36/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2587 - accuracy: 0.8916 - val_loss: 0.2761 - val_accuracy: 0.9170\n",
      "Epoch 37/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2593 - accuracy: 0.8932 - val_loss: 0.2204 - val_accuracy: 0.9185\n",
      "Epoch 38/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2576 - accuracy: 0.8923 - val_loss: 0.2454 - val_accuracy: 0.9087\n",
      "Epoch 39/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2609 - accuracy: 0.8929 - val_loss: 0.3194 - val_accuracy: 0.8970\n",
      "Epoch 40/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2565 - accuracy: 0.8923 - val_loss: 0.1990 - val_accuracy: 0.9268\n",
      "Epoch 41/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2574 - accuracy: 0.8910 - val_loss: 0.2620 - val_accuracy: 0.9183\n",
      "Epoch 42/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2543 - accuracy: 0.8953 - val_loss: 0.2252 - val_accuracy: 0.9147\n",
      "Epoch 43/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2626 - accuracy: 0.8913 - val_loss: 0.1971 - val_accuracy: 0.9153\n",
      "Epoch 44/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2575 - accuracy: 0.8911 - val_loss: 0.2834 - val_accuracy: 0.9165\n",
      "Epoch 45/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2616 - accuracy: 0.8932 - val_loss: 0.2197 - val_accuracy: 0.9210\n",
      "Epoch 46/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2622 - accuracy: 0.8922 - val_loss: 0.1987 - val_accuracy: 0.9237\n",
      "Epoch 47/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2665 - accuracy: 0.8909 - val_loss: 0.2013 - val_accuracy: 0.9200\n",
      "Epoch 48/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2709 - accuracy: 0.8888 - val_loss: 0.4360 - val_accuracy: 0.7865\n",
      "Epoch 49/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2675 - accuracy: 0.8871 - val_loss: 0.2385 - val_accuracy: 0.9230\n",
      "Epoch 50/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2662 - accuracy: 0.8886 - val_loss: 0.2698 - val_accuracy: 0.8900\n",
      "Epoch 51/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2606 - accuracy: 0.8929 - val_loss: 0.2795 - val_accuracy: 0.9030\n",
      "Epoch 52/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2708 - accuracy: 0.8872 - val_loss: 0.3879 - val_accuracy: 0.9118\n",
      "Epoch 53/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2671 - accuracy: 0.8914 - val_loss: 0.2433 - val_accuracy: 0.9118\n",
      "Epoch 54/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2687 - accuracy: 0.8925 - val_loss: 0.2959 - val_accuracy: 0.8685\n",
      "Epoch 55/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2729 - accuracy: 0.8861 - val_loss: 0.2551 - val_accuracy: 0.9003\n",
      "Epoch 56/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2738 - accuracy: 0.8855 - val_loss: 0.2509 - val_accuracy: 0.9218\n",
      "Epoch 57/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2710 - accuracy: 0.8863 - val_loss: 0.3909 - val_accuracy: 0.9047\n",
      "Epoch 58/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2782 - accuracy: 0.8867 - val_loss: 0.2400 - val_accuracy: 0.9120\n",
      "Epoch 59/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2760 - accuracy: 0.8825 - val_loss: 0.2514 - val_accuracy: 0.9065\n",
      "Epoch 60/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2778 - accuracy: 0.8846 - val_loss: 0.2986 - val_accuracy: 0.9135\n",
      "Epoch 61/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2831 - accuracy: 0.8835 - val_loss: 0.2246 - val_accuracy: 0.9170\n",
      "Epoch 62/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2861 - accuracy: 0.8823 - val_loss: 0.3556 - val_accuracy: 0.8497\n",
      "Epoch 63/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2808 - accuracy: 0.8815 - val_loss: 0.2373 - val_accuracy: 0.9208\n",
      "Epoch 64/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2773 - accuracy: 0.8864 - val_loss: 0.2710 - val_accuracy: 0.9020\n",
      "Epoch 65/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2790 - accuracy: 0.8845 - val_loss: 1.1717 - val_accuracy: 0.8677\n",
      "Epoch 66/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2800 - accuracy: 0.8829 - val_loss: 0.3125 - val_accuracy: 0.9162\n",
      "Epoch 67/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2816 - accuracy: 0.8819 - val_loss: 0.3673 - val_accuracy: 0.9120\n",
      "Epoch 68/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2808 - accuracy: 0.8821 - val_loss: 0.2670 - val_accuracy: 0.9300\n",
      "Epoch 69/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2857 - accuracy: 0.8818 - val_loss: 0.2909 - val_accuracy: 0.9095\n",
      "Epoch 70/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2849 - accuracy: 0.8809 - val_loss: 0.2974 - val_accuracy: 0.9170\n",
      "Epoch 71/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2927 - accuracy: 0.8791 - val_loss: 0.3449 - val_accuracy: 0.8687\n",
      "Epoch 72/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2903 - accuracy: 0.8792 - val_loss: 0.2622 - val_accuracy: 0.9147\n",
      "Epoch 73/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2951 - accuracy: 0.8739 - val_loss: 0.3313 - val_accuracy: 0.9168\n",
      "Epoch 74/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2823 - accuracy: 0.8816 - val_loss: 0.2657 - val_accuracy: 0.9158\n",
      "Epoch 75/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2960 - accuracy: 0.8750 - val_loss: 0.3468 - val_accuracy: 0.8982\n",
      "Epoch 76/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2954 - accuracy: 0.8751 - val_loss: 0.3686 - val_accuracy: 0.9015\n",
      "Epoch 77/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2939 - accuracy: 0.8775 - val_loss: 0.3117 - val_accuracy: 0.9285\n",
      "Epoch 78/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2994 - accuracy: 0.8781 - val_loss: 0.3707 - val_accuracy: 0.8643\n",
      "Epoch 79/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2989 - accuracy: 0.8746 - val_loss: 0.3807 - val_accuracy: 0.8825\n",
      "Epoch 80/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2843 - accuracy: 0.8794 - val_loss: 0.2944 - val_accuracy: 0.8935\n",
      "Epoch 81/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2985 - accuracy: 0.8745 - val_loss: 0.3270 - val_accuracy: 0.8865\n",
      "Epoch 82/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2997 - accuracy: 0.8723 - val_loss: 0.3475 - val_accuracy: 0.8622\n",
      "Epoch 83/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.2994 - accuracy: 0.8744 - val_loss: 0.2554 - val_accuracy: 0.9170\n",
      "Epoch 84/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3060 - accuracy: 0.8738 - val_loss: 0.3227 - val_accuracy: 0.8745\n",
      "Epoch 85/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2957 - accuracy: 0.8777 - val_loss: 0.2938 - val_accuracy: 0.8620\n",
      "Epoch 86/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2967 - accuracy: 0.8755 - val_loss: 0.2676 - val_accuracy: 0.9150\n",
      "Epoch 87/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2909 - accuracy: 0.8749 - val_loss: 0.2497 - val_accuracy: 0.9222\n",
      "Epoch 88/600\n",
      "600/600 [==============================] - 92s 152ms/step - loss: 0.2935 - accuracy: 0.8758 - val_loss: 0.6123 - val_accuracy: 0.9082\n",
      "Epoch 89/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3052 - accuracy: 0.8766 - val_loss: 0.3099 - val_accuracy: 0.9093\n",
      "Epoch 90/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2978 - accuracy: 0.8767 - val_loss: 0.3415 - val_accuracy: 0.9150\n",
      "Epoch 91/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3028 - accuracy: 0.8737 - val_loss: 0.3030 - val_accuracy: 0.9200\n",
      "Epoch 92/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2963 - accuracy: 0.8768 - val_loss: 0.2486 - val_accuracy: 0.9085\n",
      "Epoch 93/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.3141 - accuracy: 0.8732 - val_loss: 0.2667 - val_accuracy: 0.8823\n",
      "Epoch 94/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3285 - accuracy: 0.8758 - val_loss: 0.3744 - val_accuracy: 0.8668\n",
      "Epoch 95/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2965 - accuracy: 0.8764 - val_loss: 0.2917 - val_accuracy: 0.9028\n",
      "Epoch 96/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2982 - accuracy: 0.8753 - val_loss: 0.2625 - val_accuracy: 0.9085\n",
      "Epoch 97/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.3086 - accuracy: 0.8754 - val_loss: 0.2809 - val_accuracy: 0.8907\n",
      "Epoch 98/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3240 - accuracy: 0.8726 - val_loss: 0.3068 - val_accuracy: 0.9050\n",
      "Epoch 99/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2921 - accuracy: 0.8773 - val_loss: 0.3270 - val_accuracy: 0.9195\n",
      "Epoch 100/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3031 - accuracy: 0.8714 - val_loss: 0.3358 - val_accuracy: 0.9090\n",
      "Epoch 101/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2984 - accuracy: 0.8767 - val_loss: 0.2881 - val_accuracy: 0.9100\n",
      "Epoch 102/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3014 - accuracy: 0.8739 - val_loss: 0.3260 - val_accuracy: 0.8925\n",
      "Epoch 103/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2927 - accuracy: 0.8788 - val_loss: 0.2660 - val_accuracy: 0.8932\n",
      "Epoch 104/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2881 - accuracy: 0.8784 - val_loss: 0.3619 - val_accuracy: 0.8720\n",
      "Epoch 105/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.3090 - accuracy: 0.8693 - val_loss: 0.2306 - val_accuracy: 0.9160\n",
      "Epoch 106/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3341 - accuracy: 0.8772 - val_loss: 0.2657 - val_accuracy: 0.9120\n",
      "Epoch 107/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.3014 - accuracy: 0.8784 - val_loss: 0.2363 - val_accuracy: 0.9172\n",
      "Epoch 108/600\n",
      "600/600 [==============================] - 89s 149ms/step - loss: 0.3016 - accuracy: 0.8789 - val_loss: 0.2884 - val_accuracy: 0.9003\n",
      "Epoch 109/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2933 - accuracy: 0.8773 - val_loss: 0.2908 - val_accuracy: 0.9202\n",
      "Epoch 110/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.3015 - accuracy: 0.8801 - val_loss: 0.3654 - val_accuracy: 0.9057\n",
      "Epoch 111/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2925 - accuracy: 0.8785 - val_loss: 0.2765 - val_accuracy: 0.9137\n",
      "Epoch 112/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2967 - accuracy: 0.8780 - val_loss: 0.3763 - val_accuracy: 0.9178\n",
      "Epoch 113/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2920 - accuracy: 0.8771 - val_loss: 0.5021 - val_accuracy: 0.9072\n",
      "Epoch 114/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2946 - accuracy: 0.8815 - val_loss: 0.2644 - val_accuracy: 0.9243\n",
      "Epoch 115/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2955 - accuracy: 0.8828 - val_loss: 0.5552 - val_accuracy: 0.8215\n",
      "Epoch 116/600\n",
      "600/600 [==============================] - 89s 148ms/step - loss: 0.2965 - accuracy: 0.8822 - val_loss: 0.2334 - val_accuracy: 0.9190\n",
      "Epoch 117/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2870 - accuracy: 0.8845 - val_loss: 0.3282 - val_accuracy: 0.9293\n",
      "Epoch 118/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3108 - accuracy: 0.8802 - val_loss: 0.2248 - val_accuracy: 0.9275\n",
      "Epoch 119/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3012 - accuracy: 0.8813 - val_loss: 0.2926 - val_accuracy: 0.8923\n",
      "Epoch 120/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2820 - accuracy: 0.8808 - val_loss: 0.2668 - val_accuracy: 0.9168\n",
      "Epoch 121/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3075 - accuracy: 0.8761 - val_loss: 0.2677 - val_accuracy: 0.9020\n",
      "Epoch 122/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2925 - accuracy: 0.8783 - val_loss: 0.2922 - val_accuracy: 0.9180\n",
      "Epoch 123/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2946 - accuracy: 0.8827 - val_loss: 0.2059 - val_accuracy: 0.9290\n",
      "Epoch 124/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2829 - accuracy: 0.8842 - val_loss: 0.2591 - val_accuracy: 0.9250\n",
      "Epoch 125/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2862 - accuracy: 0.8830 - val_loss: 0.2103 - val_accuracy: 0.9205\n",
      "Epoch 126/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2888 - accuracy: 0.8799 - val_loss: 0.2607 - val_accuracy: 0.9245\n",
      "Epoch 127/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3271 - accuracy: 0.8800 - val_loss: 0.1944 - val_accuracy: 0.9225\n",
      "Epoch 128/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2785 - accuracy: 0.8822 - val_loss: 0.2422 - val_accuracy: 0.9215\n",
      "Epoch 129/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2899 - accuracy: 0.8807 - val_loss: 0.2796 - val_accuracy: 0.8903\n",
      "Epoch 130/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.3162 - accuracy: 0.8829 - val_loss: 0.3130 - val_accuracy: 0.9210\n",
      "Epoch 131/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2952 - accuracy: 0.8768 - val_loss: 0.2530 - val_accuracy: 0.9105\n",
      "Epoch 132/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2953 - accuracy: 0.8845 - val_loss: 0.2410 - val_accuracy: 0.9145\n",
      "Epoch 133/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2889 - accuracy: 0.8766 - val_loss: 0.2205 - val_accuracy: 0.9147\n",
      "Epoch 134/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2826 - accuracy: 0.8827 - val_loss: 0.2387 - val_accuracy: 0.9190\n",
      "Epoch 135/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2906 - accuracy: 0.8828 - val_loss: 0.3200 - val_accuracy: 0.8990\n",
      "Epoch 136/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2971 - accuracy: 0.8808 - val_loss: 0.1936 - val_accuracy: 0.9285\n",
      "Epoch 137/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2793 - accuracy: 0.8829 - val_loss: 0.2822 - val_accuracy: 0.9107\n",
      "Epoch 138/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2840 - accuracy: 0.8819 - val_loss: 0.3572 - val_accuracy: 0.8882\n",
      "Epoch 139/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2830 - accuracy: 0.8815 - val_loss: 0.2529 - val_accuracy: 0.8972\n",
      "Epoch 140/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 2.5390 - accuracy: 0.8829 - val_loss: 0.1994 - val_accuracy: 0.9333\n",
      "Epoch 141/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2903 - accuracy: 0.8763 - val_loss: 0.2489 - val_accuracy: 0.9260\n",
      "Epoch 142/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2825 - accuracy: 0.8807 - val_loss: 0.2995 - val_accuracy: 0.9103\n",
      "Epoch 143/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2843 - accuracy: 0.8823 - val_loss: 0.2349 - val_accuracy: 0.9120\n",
      "Epoch 144/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2872 - accuracy: 0.8828 - val_loss: 0.2232 - val_accuracy: 0.9252\n",
      "Epoch 145/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2836 - accuracy: 0.8803 - val_loss: 0.2367 - val_accuracy: 0.9120\n",
      "Epoch 146/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3044 - accuracy: 0.8808 - val_loss: 0.2752 - val_accuracy: 0.9240\n",
      "Epoch 147/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2795 - accuracy: 0.8846 - val_loss: 0.2719 - val_accuracy: 0.9045\n",
      "Epoch 148/600\n",
      "600/600 [==============================] - 93s 154ms/step - loss: 0.2952 - accuracy: 0.8819 - val_loss: 0.3229 - val_accuracy: 0.9118\n",
      "Epoch 149/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2866 - accuracy: 0.8813 - val_loss: 0.2881 - val_accuracy: 0.8855\n",
      "Epoch 150/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2928 - accuracy: 0.8809 - val_loss: 0.2390 - val_accuracy: 0.9162\n",
      "Epoch 151/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2876 - accuracy: 0.8799 - val_loss: 0.2988 - val_accuracy: 0.9120\n",
      "Epoch 152/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2959 - accuracy: 0.8768 - val_loss: 0.2329 - val_accuracy: 0.9097\n",
      "Epoch 153/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2827 - accuracy: 0.8823 - val_loss: 0.2598 - val_accuracy: 0.9072\n",
      "Epoch 154/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3029 - accuracy: 0.8786 - val_loss: 0.3067 - val_accuracy: 0.9158\n",
      "Epoch 155/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2943 - accuracy: 0.8794 - val_loss: 0.2798 - val_accuracy: 0.9155\n",
      "Epoch 156/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2939 - accuracy: 0.8803 - val_loss: 0.2284 - val_accuracy: 0.9197\n",
      "Epoch 157/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3004 - accuracy: 0.8804 - val_loss: 0.2845 - val_accuracy: 0.9020\n",
      "Epoch 158/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2947 - accuracy: 0.8777 - val_loss: 0.2905 - val_accuracy: 0.8942\n",
      "Epoch 159/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2901 - accuracy: 0.8791 - val_loss: 0.2941 - val_accuracy: 0.8995\n",
      "Epoch 160/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3189 - accuracy: 0.8807 - val_loss: 0.3557 - val_accuracy: 0.8972\n",
      "Epoch 161/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3030 - accuracy: 0.8805 - val_loss: 0.2903 - val_accuracy: 0.9265\n",
      "Epoch 162/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2867 - accuracy: 0.8837 - val_loss: 0.4340 - val_accuracy: 0.9122\n",
      "Epoch 163/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.4991 - accuracy: 0.8786 - val_loss: 0.3304 - val_accuracy: 0.8945\n",
      "Epoch 164/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2765 - accuracy: 0.8818 - val_loss: 0.2886 - val_accuracy: 0.9290\n",
      "Epoch 165/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2960 - accuracy: 0.8799 - val_loss: 0.3070 - val_accuracy: 0.8608\n",
      "Epoch 166/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2885 - accuracy: 0.8826 - val_loss: 0.3867 - val_accuracy: 0.8930\n",
      "Epoch 167/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2850 - accuracy: 0.8830 - val_loss: 0.2911 - val_accuracy: 0.8715\n",
      "Epoch 168/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2877 - accuracy: 0.8791 - val_loss: 0.2296 - val_accuracy: 0.9115\n",
      "Epoch 169/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2846 - accuracy: 0.8815 - val_loss: 0.3224 - val_accuracy: 0.9040\n",
      "Epoch 170/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2939 - accuracy: 0.8794 - val_loss: 0.3467 - val_accuracy: 0.9047\n",
      "Epoch 171/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2892 - accuracy: 0.8784 - val_loss: 0.3189 - val_accuracy: 0.8587\n",
      "Epoch 172/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2949 - accuracy: 0.8792 - val_loss: 0.3656 - val_accuracy: 0.9252\n",
      "Epoch 173/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2937 - accuracy: 0.8759 - val_loss: 0.2464 - val_accuracy: 0.9218\n",
      "Epoch 174/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2868 - accuracy: 0.8814 - val_loss: 0.2303 - val_accuracy: 0.9215\n",
      "Epoch 175/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2845 - accuracy: 0.8818 - val_loss: 0.2862 - val_accuracy: 0.9095\n",
      "Epoch 176/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3003 - accuracy: 0.8787 - val_loss: 0.2598 - val_accuracy: 0.9195\n",
      "Epoch 177/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2970 - accuracy: 0.8813 - val_loss: 0.4404 - val_accuracy: 0.8455\n",
      "Epoch 178/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2972 - accuracy: 0.8791 - val_loss: 0.2290 - val_accuracy: 0.9155\n",
      "Epoch 179/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2814 - accuracy: 0.8824 - val_loss: 0.4516 - val_accuracy: 0.8155\n",
      "Epoch 180/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3053 - accuracy: 0.8814 - val_loss: 0.2926 - val_accuracy: 0.9003\n",
      "Epoch 181/600\n",
      "600/600 [==============================] - 92s 152ms/step - loss: 0.2921 - accuracy: 0.8818 - val_loss: 0.2348 - val_accuracy: 0.9160\n",
      "Epoch 182/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2869 - accuracy: 0.8797 - val_loss: 0.1915 - val_accuracy: 0.9227\n",
      "Epoch 183/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2958 - accuracy: 0.8798 - val_loss: 0.3300 - val_accuracy: 0.8508\n",
      "Epoch 184/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2867 - accuracy: 0.8810 - val_loss: 0.3316 - val_accuracy: 0.9003\n",
      "Epoch 185/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2859 - accuracy: 0.8812 - val_loss: 0.2500 - val_accuracy: 0.9243\n",
      "Epoch 186/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2927 - accuracy: 0.8819 - val_loss: 0.3243 - val_accuracy: 0.9025\n",
      "Epoch 187/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2970 - accuracy: 0.8788 - val_loss: 0.3266 - val_accuracy: 0.9175\n",
      "Epoch 188/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2876 - accuracy: 0.8804 - val_loss: 0.2677 - val_accuracy: 0.8928\n",
      "Epoch 189/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2813 - accuracy: 0.8816 - val_loss: 1.1915 - val_accuracy: 0.8462\n",
      "Epoch 190/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2989 - accuracy: 0.8793 - val_loss: 0.2709 - val_accuracy: 0.9240\n",
      "Epoch 191/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2790 - accuracy: 0.8842 - val_loss: 0.4497 - val_accuracy: 0.8683\n",
      "Epoch 192/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2982 - accuracy: 0.8766 - val_loss: 0.2935 - val_accuracy: 0.9032\n",
      "Epoch 193/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2965 - accuracy: 0.8779 - val_loss: 0.2313 - val_accuracy: 0.9178\n",
      "Epoch 194/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2917 - accuracy: 0.8834 - val_loss: 0.2235 - val_accuracy: 0.9195\n",
      "Epoch 195/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2865 - accuracy: 0.8865 - val_loss: 0.2046 - val_accuracy: 0.9335\n",
      "Epoch 196/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2947 - accuracy: 0.8876 - val_loss: 0.2380 - val_accuracy: 0.9165\n",
      "Epoch 197/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2846 - accuracy: 0.8821 - val_loss: 0.2336 - val_accuracy: 0.9252\n",
      "Epoch 198/600\n",
      "600/600 [==============================] - 93s 154ms/step - loss: 0.2904 - accuracy: 0.8837 - val_loss: 0.2004 - val_accuracy: 0.9243\n",
      "Epoch 199/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2924 - accuracy: 0.8794 - val_loss: 0.2269 - val_accuracy: 0.9110\n",
      "Epoch 200/600\n",
      "600/600 [==============================] - 90s 149ms/step - loss: 0.2792 - accuracy: 0.8826 - val_loss: 0.2634 - val_accuracy: 0.9168\n",
      "Epoch 201/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2793 - accuracy: 0.8845 - val_loss: 0.2367 - val_accuracy: 0.9247\n",
      "Epoch 202/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3242 - accuracy: 0.8891 - val_loss: 0.3777 - val_accuracy: 0.8903\n",
      "Epoch 203/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.3045 - accuracy: 0.8833 - val_loss: 0.2383 - val_accuracy: 0.9130\n",
      "Epoch 204/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2782 - accuracy: 0.8824 - val_loss: 0.2454 - val_accuracy: 0.9185\n",
      "Epoch 205/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 8.6340 - accuracy: 0.8838 - val_loss: 0.2447 - val_accuracy: 0.9215\n",
      "Epoch 206/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2738 - accuracy: 0.8827 - val_loss: 0.2673 - val_accuracy: 0.9045\n",
      "Epoch 207/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3082 - accuracy: 0.8809 - val_loss: 0.2573 - val_accuracy: 0.9225\n",
      "Epoch 208/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2817 - accuracy: 0.8822 - val_loss: 0.2335 - val_accuracy: 0.9285\n",
      "Epoch 209/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2817 - accuracy: 0.8835 - val_loss: 0.2392 - val_accuracy: 0.9200\n",
      "Epoch 210/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2848 - accuracy: 0.8799 - val_loss: 0.2604 - val_accuracy: 0.8963\n",
      "Epoch 211/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2922 - accuracy: 0.8833 - val_loss: 0.2548 - val_accuracy: 0.9197\n",
      "Epoch 212/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 1.7639 - accuracy: 0.8875 - val_loss: 0.2301 - val_accuracy: 0.9172\n",
      "Epoch 213/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2863 - accuracy: 0.8828 - val_loss: 0.2007 - val_accuracy: 0.9237\n",
      "Epoch 214/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2755 - accuracy: 0.8843 - val_loss: 0.2990 - val_accuracy: 0.8760\n",
      "Epoch 215/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2953 - accuracy: 0.8801 - val_loss: 0.2098 - val_accuracy: 0.9220\n",
      "Epoch 216/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3026 - accuracy: 0.8853 - val_loss: 0.2089 - val_accuracy: 0.9310\n",
      "Epoch 217/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2844 - accuracy: 0.8809 - val_loss: 0.2608 - val_accuracy: 0.9260\n",
      "Epoch 218/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2854 - accuracy: 0.8827 - val_loss: 0.2239 - val_accuracy: 0.9345\n",
      "Epoch 219/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2852 - accuracy: 0.8824 - val_loss: 0.2414 - val_accuracy: 0.9147\n",
      "Epoch 220/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2926 - accuracy: 0.8794 - val_loss: 0.2352 - val_accuracy: 0.9107\n",
      "Epoch 221/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.4199 - accuracy: 0.8837 - val_loss: 0.4456 - val_accuracy: 0.8430\n",
      "Epoch 222/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2771 - accuracy: 0.8849 - val_loss: 0.2008 - val_accuracy: 0.9200\n",
      "Epoch 223/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2857 - accuracy: 0.8831 - val_loss: 0.2269 - val_accuracy: 0.9172\n",
      "Epoch 224/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2821 - accuracy: 0.8818 - val_loss: 0.2662 - val_accuracy: 0.8910\n",
      "Epoch 225/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2813 - accuracy: 0.8841 - val_loss: 0.2491 - val_accuracy: 0.9087\n",
      "Epoch 226/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2861 - accuracy: 0.8849 - val_loss: 0.2106 - val_accuracy: 0.9233\n",
      "Epoch 227/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2805 - accuracy: 0.8832 - val_loss: 0.3298 - val_accuracy: 0.8930\n",
      "Epoch 228/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2716 - accuracy: 0.8859 - val_loss: 0.2033 - val_accuracy: 0.9293\n",
      "Epoch 229/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2775 - accuracy: 0.8834 - val_loss: 0.2285 - val_accuracy: 0.9135\n",
      "Epoch 230/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2807 - accuracy: 0.8867 - val_loss: 0.2284 - val_accuracy: 0.8990\n",
      "Epoch 231/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3744 - accuracy: 0.8873 - val_loss: 0.2503 - val_accuracy: 0.9193\n",
      "Epoch 232/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2996 - accuracy: 0.8857 - val_loss: 0.3322 - val_accuracy: 0.9193\n",
      "Epoch 233/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2714 - accuracy: 0.8891 - val_loss: 0.2402 - val_accuracy: 0.9140\n",
      "Epoch 234/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2825 - accuracy: 0.8824 - val_loss: 0.2116 - val_accuracy: 0.9190\n",
      "Epoch 235/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2808 - accuracy: 0.8822 - val_loss: 0.2539 - val_accuracy: 0.9268\n",
      "Epoch 236/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2772 - accuracy: 0.8857 - val_loss: 0.2595 - val_accuracy: 0.9105\n",
      "Epoch 237/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2860 - accuracy: 0.8857 - val_loss: 0.2985 - val_accuracy: 0.9208\n",
      "Epoch 238/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2820 - accuracy: 0.8845 - val_loss: 0.2999 - val_accuracy: 0.9060\n",
      "Epoch 239/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2826 - accuracy: 0.8853 - val_loss: 0.1911 - val_accuracy: 0.9270\n",
      "Epoch 240/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2753 - accuracy: 0.8856 - val_loss: 0.2259 - val_accuracy: 0.9183\n",
      "Epoch 241/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2792 - accuracy: 0.8854 - val_loss: 0.2660 - val_accuracy: 0.9005\n",
      "Epoch 242/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 2.2016 - accuracy: 0.8823 - val_loss: 0.2378 - val_accuracy: 0.9178\n",
      "Epoch 243/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2716 - accuracy: 0.8872 - val_loss: 0.2737 - val_accuracy: 0.9220\n",
      "Epoch 244/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2973 - accuracy: 0.8867 - val_loss: 0.2246 - val_accuracy: 0.9320\n",
      "Epoch 245/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3252 - accuracy: 0.8856 - val_loss: 0.3076 - val_accuracy: 0.8558\n",
      "Epoch 246/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2805 - accuracy: 0.8819 - val_loss: 0.2924 - val_accuracy: 0.9195\n",
      "Epoch 247/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2768 - accuracy: 0.8832 - val_loss: 0.3047 - val_accuracy: 0.9030\n",
      "Epoch 248/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2964 - accuracy: 0.8817 - val_loss: 0.2273 - val_accuracy: 0.9202\n",
      "Epoch 249/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2900 - accuracy: 0.8866 - val_loss: 0.2416 - val_accuracy: 0.9265\n",
      "Epoch 250/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2792 - accuracy: 0.8861 - val_loss: 0.2019 - val_accuracy: 0.9315\n",
      "Epoch 251/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2721 - accuracy: 0.8871 - val_loss: 0.4939 - val_accuracy: 0.8702\n",
      "Epoch 252/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2714 - accuracy: 0.8876 - val_loss: 0.2582 - val_accuracy: 0.9122\n",
      "Epoch 253/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2890 - accuracy: 0.8832 - val_loss: 0.2413 - val_accuracy: 0.9060\n",
      "Epoch 254/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2844 - accuracy: 0.8916 - val_loss: 0.3156 - val_accuracy: 0.8910\n",
      "Epoch 255/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2800 - accuracy: 0.8868 - val_loss: 0.2330 - val_accuracy: 0.9202\n",
      "Epoch 256/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2790 - accuracy: 0.8838 - val_loss: 0.2456 - val_accuracy: 0.9202\n",
      "Epoch 257/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2678 - accuracy: 0.8878 - val_loss: 0.2256 - val_accuracy: 0.9103\n",
      "Epoch 258/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2651 - accuracy: 0.8864 - val_loss: 0.2485 - val_accuracy: 0.9018\n",
      "Epoch 259/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2858 - accuracy: 0.8796 - val_loss: 0.3108 - val_accuracy: 0.9035\n",
      "Epoch 260/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3023 - accuracy: 0.8852 - val_loss: 0.2180 - val_accuracy: 0.9105\n",
      "Epoch 261/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2753 - accuracy: 0.8876 - val_loss: 0.3119 - val_accuracy: 0.8972\n",
      "Epoch 262/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2933 - accuracy: 0.8834 - val_loss: 0.2082 - val_accuracy: 0.9215\n",
      "Epoch 263/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2806 - accuracy: 0.8831 - val_loss: 0.2384 - val_accuracy: 0.9190\n",
      "Epoch 264/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2704 - accuracy: 0.8868 - val_loss: 0.2198 - val_accuracy: 0.9315\n",
      "Epoch 265/600\n",
      "600/600 [==============================] - 93s 154ms/step - loss: 0.2984 - accuracy: 0.8883 - val_loss: 0.1883 - val_accuracy: 0.9317\n",
      "Epoch 266/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2684 - accuracy: 0.8888 - val_loss: 0.2692 - val_accuracy: 0.9022\n",
      "Epoch 267/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2775 - accuracy: 0.8876 - val_loss: 0.2392 - val_accuracy: 0.9103\n",
      "Epoch 268/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2748 - accuracy: 0.8872 - val_loss: 0.4082 - val_accuracy: 0.8055\n",
      "Epoch 269/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2957 - accuracy: 0.8841 - val_loss: 0.4254 - val_accuracy: 0.8500\n",
      "Epoch 270/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2778 - accuracy: 0.8867 - val_loss: 0.3076 - val_accuracy: 0.8980\n",
      "Epoch 271/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2919 - accuracy: 0.8859 - val_loss: 0.2002 - val_accuracy: 0.9230\n",
      "Epoch 272/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.3464 - accuracy: 0.8854 - val_loss: 0.3293 - val_accuracy: 0.8685\n",
      "Epoch 273/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2876 - accuracy: 0.8849 - val_loss: 0.3070 - val_accuracy: 0.8500\n",
      "Epoch 274/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2729 - accuracy: 0.8853 - val_loss: 0.2383 - val_accuracy: 0.9055\n",
      "Epoch 275/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2719 - accuracy: 0.8856 - val_loss: 0.1896 - val_accuracy: 0.9285\n",
      "Epoch 276/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2699 - accuracy: 0.8866 - val_loss: 0.3049 - val_accuracy: 0.9070\n",
      "Epoch 277/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2859 - accuracy: 0.8844 - val_loss: 0.2692 - val_accuracy: 0.9040\n",
      "Epoch 278/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2688 - accuracy: 0.8896 - val_loss: 0.2302 - val_accuracy: 0.9235\n",
      "Epoch 279/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.3406 - accuracy: 0.8903 - val_loss: 0.2118 - val_accuracy: 0.9200\n",
      "Epoch 280/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2894 - accuracy: 0.8844 - val_loss: 0.2364 - val_accuracy: 0.9020\n",
      "Epoch 281/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2675 - accuracy: 0.8886 - val_loss: 0.3088 - val_accuracy: 0.9137\n",
      "Epoch 282/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2750 - accuracy: 0.8829 - val_loss: 0.3220 - val_accuracy: 0.8878\n",
      "Epoch 283/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3033 - accuracy: 0.8868 - val_loss: 0.3672 - val_accuracy: 0.9103\n",
      "Epoch 284/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2761 - accuracy: 0.8913 - val_loss: 0.2088 - val_accuracy: 0.9243\n",
      "Epoch 285/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2916 - accuracy: 0.8858 - val_loss: 0.2333 - val_accuracy: 0.9162\n",
      "Epoch 286/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2862 - accuracy: 0.8879 - val_loss: 0.2074 - val_accuracy: 0.9227\n",
      "Epoch 287/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2669 - accuracy: 0.8874 - val_loss: 0.2524 - val_accuracy: 0.9053\n",
      "Epoch 288/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2780 - accuracy: 0.8841 - val_loss: 0.1999 - val_accuracy: 0.9243\n",
      "Epoch 289/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2791 - accuracy: 0.8886 - val_loss: 0.2035 - val_accuracy: 0.9350\n",
      "Epoch 290/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2797 - accuracy: 0.8859 - val_loss: 0.1999 - val_accuracy: 0.9310\n",
      "Epoch 291/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.3090 - accuracy: 0.8896 - val_loss: 0.2208 - val_accuracy: 0.9243\n",
      "Epoch 292/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3140 - accuracy: 0.8917 - val_loss: 0.3062 - val_accuracy: 0.8775\n",
      "Epoch 293/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2637 - accuracy: 0.8888 - val_loss: 0.2778 - val_accuracy: 0.8763\n",
      "Epoch 294/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2743 - accuracy: 0.8919 - val_loss: 0.2312 - val_accuracy: 0.9268\n",
      "Epoch 295/600\n",
      "600/600 [==============================] - 93s 154ms/step - loss: 1.0072 - accuracy: 0.8931 - val_loss: 0.2125 - val_accuracy: 0.9215\n",
      "Epoch 296/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2645 - accuracy: 0.8906 - val_loss: 0.3974 - val_accuracy: 0.8802\n",
      "Epoch 297/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2970 - accuracy: 0.8885 - val_loss: 0.2100 - val_accuracy: 0.9212\n",
      "Epoch 298/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2675 - accuracy: 0.8890 - val_loss: 0.2610 - val_accuracy: 0.9150\n",
      "Epoch 299/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2797 - accuracy: 0.8903 - val_loss: 0.2269 - val_accuracy: 0.9090\n",
      "Epoch 300/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2764 - accuracy: 0.8872 - val_loss: 0.2084 - val_accuracy: 0.9122\n",
      "Epoch 301/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2691 - accuracy: 0.8900 - val_loss: 0.2182 - val_accuracy: 0.9268\n",
      "Epoch 302/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2769 - accuracy: 0.8889 - val_loss: 0.2466 - val_accuracy: 0.9200\n",
      "Epoch 303/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2785 - accuracy: 0.8932 - val_loss: 0.2141 - val_accuracy: 0.9225\n",
      "Epoch 304/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3206 - accuracy: 0.8886 - val_loss: 0.2499 - val_accuracy: 0.8988\n",
      "Epoch 305/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2740 - accuracy: 0.8944 - val_loss: 0.1884 - val_accuracy: 0.9310\n",
      "Epoch 306/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2790 - accuracy: 0.8880 - val_loss: 0.2199 - val_accuracy: 0.9190\n",
      "Epoch 307/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2744 - accuracy: 0.8871 - val_loss: 0.2320 - val_accuracy: 0.9255\n",
      "Epoch 308/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2692 - accuracy: 0.8899 - val_loss: 0.1876 - val_accuracy: 0.9293\n",
      "Epoch 309/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2729 - accuracy: 0.8916 - val_loss: 0.2880 - val_accuracy: 0.9158\n",
      "Epoch 310/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2714 - accuracy: 0.8917 - val_loss: 0.2965 - val_accuracy: 0.9195\n",
      "Epoch 311/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2815 - accuracy: 0.8893 - val_loss: 0.2131 - val_accuracy: 0.9285\n",
      "Epoch 312/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2683 - accuracy: 0.8902 - val_loss: 0.2298 - val_accuracy: 0.9245\n",
      "Epoch 313/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2879 - accuracy: 0.8907 - val_loss: 0.2619 - val_accuracy: 0.9308\n",
      "Epoch 314/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2679 - accuracy: 0.8876 - val_loss: 0.3921 - val_accuracy: 0.9295\n",
      "Epoch 315/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.5388 - accuracy: 0.8859 - val_loss: 0.2446 - val_accuracy: 0.9210\n",
      "Epoch 316/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2740 - accuracy: 0.8870 - val_loss: 0.2682 - val_accuracy: 0.9315\n",
      "Epoch 317/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3299 - accuracy: 0.8927 - val_loss: 0.2373 - val_accuracy: 0.9280\n",
      "Epoch 318/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2759 - accuracy: 0.8878 - val_loss: 0.3051 - val_accuracy: 0.9028\n",
      "Epoch 319/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2699 - accuracy: 0.8908 - val_loss: 0.2345 - val_accuracy: 0.9237\n",
      "Epoch 320/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2608 - accuracy: 0.8961 - val_loss: 0.4548 - val_accuracy: 0.8932\n",
      "Epoch 321/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2615 - accuracy: 0.8944 - val_loss: 0.1796 - val_accuracy: 0.9270\n",
      "Epoch 322/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2677 - accuracy: 0.8910 - val_loss: 0.1970 - val_accuracy: 0.9252\n",
      "Epoch 323/600\n",
      "600/600 [==============================] - 90s 151ms/step - loss: 0.2683 - accuracy: 0.8926 - val_loss: 0.2504 - val_accuracy: 0.9025\n",
      "Epoch 324/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2729 - accuracy: 0.8850 - val_loss: 0.2449 - val_accuracy: 0.9060\n",
      "Epoch 325/600\n",
      "600/600 [==============================] - 92s 152ms/step - loss: 0.2846 - accuracy: 0.8841 - val_loss: 0.2105 - val_accuracy: 0.9283\n",
      "Epoch 326/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2808 - accuracy: 0.8842 - val_loss: 0.2086 - val_accuracy: 0.9290\n",
      "Epoch 327/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 1.4718 - accuracy: 0.8942 - val_loss: 0.1981 - val_accuracy: 0.9340\n",
      "Epoch 328/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2623 - accuracy: 0.8905 - val_loss: 0.1935 - val_accuracy: 0.9258\n",
      "Epoch 329/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.3257 - accuracy: 0.8891 - val_loss: 0.3140 - val_accuracy: 0.8700\n",
      "Epoch 330/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2708 - accuracy: 0.8893 - val_loss: 0.1709 - val_accuracy: 0.9355\n",
      "Epoch 331/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2799 - accuracy: 0.8860 - val_loss: 0.2359 - val_accuracy: 0.9158\n",
      "Epoch 332/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2756 - accuracy: 0.8894 - val_loss: 0.2626 - val_accuracy: 0.9235\n",
      "Epoch 333/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2824 - accuracy: 0.8875 - val_loss: 0.3400 - val_accuracy: 0.8855\n",
      "Epoch 334/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2672 - accuracy: 0.8891 - val_loss: 0.2560 - val_accuracy: 0.9082\n",
      "Epoch 335/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2786 - accuracy: 0.8880 - val_loss: 0.2172 - val_accuracy: 0.9310\n",
      "Epoch 336/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2892 - accuracy: 0.8849 - val_loss: 0.5424 - val_accuracy: 0.8430\n",
      "Epoch 337/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2779 - accuracy: 0.8908 - val_loss: 0.3225 - val_accuracy: 0.9235\n",
      "Epoch 338/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2780 - accuracy: 0.8874 - val_loss: 0.2168 - val_accuracy: 0.9280\n",
      "Epoch 339/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.4430 - accuracy: 0.8932 - val_loss: 0.4628 - val_accuracy: 0.9168\n",
      "Epoch 340/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2750 - accuracy: 0.8899 - val_loss: 0.2098 - val_accuracy: 0.9222\n",
      "Epoch 341/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2851 - accuracy: 0.8873 - val_loss: 0.2865 - val_accuracy: 0.9230\n",
      "Epoch 342/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2735 - accuracy: 0.8896 - val_loss: 0.1806 - val_accuracy: 0.9323\n",
      "Epoch 343/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2749 - accuracy: 0.8887 - val_loss: 0.2220 - val_accuracy: 0.9178\n",
      "Epoch 344/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2745 - accuracy: 0.8871 - val_loss: 0.2382 - val_accuracy: 0.9043\n",
      "Epoch 345/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2994 - accuracy: 0.8910 - val_loss: 0.2510 - val_accuracy: 0.9222\n",
      "Epoch 346/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.7951 - accuracy: 0.8892 - val_loss: 0.2521 - val_accuracy: 0.9300\n",
      "Epoch 347/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2608 - accuracy: 0.8918 - val_loss: 0.3025 - val_accuracy: 0.9082\n",
      "Epoch 348/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2729 - accuracy: 0.8870 - val_loss: 0.4768 - val_accuracy: 0.9090\n",
      "Epoch 349/600\n",
      "600/600 [==============================] - 93s 155ms/step - loss: 0.2644 - accuracy: 0.8884 - val_loss: 0.4299 - val_accuracy: 0.8825\n",
      "Epoch 350/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2855 - accuracy: 0.8911 - val_loss: 0.2881 - val_accuracy: 0.9247\n",
      "Epoch 351/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2699 - accuracy: 0.8885 - val_loss: 0.2415 - val_accuracy: 0.9283\n",
      "Epoch 352/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2739 - accuracy: 0.8905 - val_loss: 0.4663 - val_accuracy: 0.9295\n",
      "Epoch 353/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2823 - accuracy: 0.8911 - val_loss: 0.2563 - val_accuracy: 0.9105\n",
      "Epoch 354/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2675 - accuracy: 0.8942 - val_loss: 0.2750 - val_accuracy: 0.9193\n",
      "Epoch 355/600\n",
      "600/600 [==============================] - 92s 154ms/step - loss: 0.2666 - accuracy: 0.8863 - val_loss: 0.2472 - val_accuracy: 0.9265\n",
      "Epoch 356/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.3031 - accuracy: 0.8866 - val_loss: 0.2535 - val_accuracy: 0.9040\n",
      "Epoch 357/600\n",
      "600/600 [==============================] - 91s 152ms/step - loss: 0.2794 - accuracy: 0.8913 - val_loss: 0.2801 - val_accuracy: 0.9133\n",
      "Epoch 358/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2663 - accuracy: 0.8936 - val_loss: 0.1978 - val_accuracy: 0.9340\n",
      "Epoch 359/600\n",
      "600/600 [==============================] - 90s 150ms/step - loss: 0.2731 - accuracy: 0.8879 - val_loss: 0.2327 - val_accuracy: 0.9243\n",
      "Epoch 360/600\n",
      "600/600 [==============================] - 91s 151ms/step - loss: 0.2602 - accuracy: 0.8923 - val_loss: 0.2631 - val_accuracy: 0.9070\n",
      "Epoch 361/600\n",
      "600/600 [==============================] - 92s 153ms/step - loss: 0.2764 - accuracy: 0.8876 - val_loss: 0.3166 - val_accuracy: 0.9235\n",
      "Epoch 362/600\n",
      "479/600 [======================>.......] - ETA: 17s - loss: 0.2757 - accuracy: 0.8933"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4564\\3945102299.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model.fit(train_generator,\n\u001b[0m\u001b[0;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     validation_data = valid_generator)\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "history = model.fit(train_generator,\n",
    "                    epochs = 600,\n",
    "                    validation_data = valid_generator)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('Czas: {}'.format(toc-tic))\n",
    "\n",
    "                              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48f04b-b403-4429-8682-54717c7c2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161c348-57cc-4529-8570-d146e83ad168",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "test_generator = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                                 target_size=(150,150),\n",
    "                                                  batch_size = 20,\n",
    "                                                  class_mode='binary')\n",
    "\n",
    "test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a39037-bac9-4d4b-9344-ee70f233e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ec190-7f5e-4062-a9d6-4777d1340306",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =model.predict(test_generator).apply(lambda:x 1 if x>0.5 else:0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825654c-58c1-466a-a518-fd8009b950f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_accuracy_plot(history):\n",
    "    \"\"\"\n",
    "    Funkcja zwraca wykres dokadnoci (accuracy) modelu na zbiorze treningowym\n",
    "    i walidacyjnym.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    acc, val_acc = history.history['accuracy'], history.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(epochs, acc, label='Dokadno trenowania', marker='o')\n",
    "    plt.plot(epochs, val_acc, label='Dokadno walidacji', marker='o')\n",
    "    plt.legend()\n",
    "    plt.title('Dokadno trenowania i walidacji')\n",
    "    plt.xlabel('Epoki')\n",
    "    plt.ylabel('Dokadno')\n",
    "    plt.show()\n",
    "\n",
    "def make_loss_plot(history):\n",
    "    \"\"\"\n",
    "    Funkcja zwraca wykres straty (loss) modelu na zbiorze treningowym\n",
    "    i walidacyjnym.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    loss, val_loss = history.history['loss'], history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(epochs, loss, label='Strata trenowania', marker='o')\n",
    "    plt.plot(epochs, val_loss, label='Strata walidacji', marker='o')\n",
    "    plt.legend()\n",
    "    plt.title('Strata trenowania i walidacji')\n",
    "    plt.xlabel('Epoki')\n",
    "    plt.ylabel('Strata')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e6cd1-360b-4cc7-bc10-f04e339e552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_accuracy_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374ac13-24e9-4aef-8c49-43042a81f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_loss_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12b455-7415-4bef-b86d-0d3b7c270f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
